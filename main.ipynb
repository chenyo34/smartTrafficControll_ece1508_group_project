{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sumo_rl import SumoEnvironment\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo\n"
     ]
    }
   ],
   "source": [
    "print(\"SUMO HOME:\", os.environ.get(\"SUMO_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Check the actions and the state space =====\n",
    "# print(\"Action Space:\", env.action_space)\n",
    "# print(\"State Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Demo skipped.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# ========= Simple simulation test of  SUMO-RL ===========\n",
    "\n",
    "demo_trigger = input(\"Do you want to run the demo?\")\n",
    "if demo_trigger.lower() in ['y', 'yes']:\n",
    "    print(\"Demo Starting....\")\n",
    "    exit()\n",
    "\n",
    "    # File paths\n",
    "    NET_FILE = 'single-intersection.net.xml'      # network file\n",
    "    ROU_FILE = 'single-intersection-vertical.rou.xml'  # route file (traffic flows)\n",
    "\n",
    "    # Initialize SUMO environment\n",
    "    env = SumoEnvironment(\n",
    "        net_file=NET_FILE,\n",
    "        route_file=ROU_FILE,\n",
    "        use_gui=False,                 # show SUMO GUI\n",
    "        single_agent=True,            # one intersection controller\n",
    "        reward_fn='diff-waiting-time',\n",
    "        out_csv_name='outputs/traffic_log'\n",
    "    )\n",
    "\n",
    "    # Intialization\n",
    "    obs, info = env.reset()\n",
    "    avg_speed_per_road, avg_wait_time_per_road = np.zeros(4), np.zeros(4)\n",
    "    avg_pressure, avg_total_speed = [],[]\n",
    "    rewards, actions = [], []\n",
    "    sim_time = 0.0\n",
    "\n",
    "    for step in range(100):\n",
    "\n",
    "        print(f\"--- Step {step} / Time {sim_time:.1f} ---\")\n",
    "\n",
    "        # Take a random action\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "\n",
    "        # Recieve the env feedback\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Current simulation time\n",
    "        sim_time = env.sumo.simulation.getTime()\n",
    "\n",
    "        # Get the vehicles IDs\n",
    "        veh_ids = env.sumo.vehicle.getIDList()\n",
    "\n",
    "        # Collisions Detection\n",
    "        # collisions = env.sumo.simulation.getCollidingVehiclesIDList()\n",
    "        # if len(collisions) > 0:\n",
    "        #     print(f\"Collisions: {collisions}\")\n",
    "\n",
    "        #     # Reset env after collision\n",
    "        #     obs, info = env.reset()\n",
    "        #     print(\"Environment reset.\\n\")\n",
    "        #     break\n",
    "\n",
    "        # Integrate information from the environment\n",
    "        lane_counter = {}\n",
    "        total_speed = 0.0\n",
    "        intsec_pressure = 0.0\n",
    "\n",
    "        for vid in veh_ids:\n",
    "            speed = env.sumo.vehicle.getSpeed(vid)\n",
    "            lane = env.sumo.vehicle.getLaneID(vid)\n",
    "            # pos = env.sumo.vehicle.getPosition(vid)\n",
    "            # print(f\"{vid}: {speed:.2f} m/s on {edge} ({lane}), pos={pos}\")\n",
    "            total_speed += speed\n",
    "            lane_counter[lane] =  lane_counter.get(lane,0) + 1\n",
    "        \n",
    "        avg_speed = total_speed/len(veh_ids) if len(veh_ids) > 0 else 0\n",
    "        avg_total_speed.append(avg_speed)\n",
    "\n",
    "        print(f\"Average speed of all vehicles: {avg_speed:.2f} m/s\")\n",
    "        print(f\"Vechicles per lane:{lane_counter}\")\n",
    "\n",
    "\n",
    "        # Arrived vehicles\n",
    "        # arrived = env.sumo.simulation.getArrivedIDList()\n",
    "        # if len(arrived) > 0:\n",
    "        #     print(f\"Arrived: {arrived}\")\n",
    "\n",
    "        # Red-Green Light States\n",
    "        # for tls in env.ts_ids:\n",
    "        state = env.sumo.trafficlight.getRedYellowGreenState(env.ts_ids[0])\n",
    "        phase = env.sumo.trafficlight.getPhase(env.ts_ids[0])\n",
    "        print(f\"TraLight state=:{state} \")\n",
    "\n",
    "        # Reward\n",
    "        print(f\"Step reward: {reward:.3f}\\n\")\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            print(\"Environment reset.\\n\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"=\"*20)\n",
    "    print(\"Demo Simulation finished!\")\n",
    "    print(\"=\"*20)\n",
    "else:\n",
    "    print(\"=\"*20)\n",
    "    print(\"Demo skipped.\")\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "from torch import nn\n",
    "\n",
    "# from torch.nn import function as F\n",
    "\n",
    "def train_ppo(\n",
    "    model,\n",
    "    env=None,\n",
    "    GAMMA = 0.99,\n",
    "    GAE_LAMBDA = 0.95,\n",
    "    CLIP_EPS = 0.2,\n",
    "    LR = 3e-3,\n",
    "    ENT_COEF = 0.01,\n",
    "    VF_COEF = 0.5,\n",
    "    MAX_GRAD_NORM = 0.5,\n",
    "    N_STEPS = 256, \n",
    "    N_EPOCHS = 10,        \n",
    "    MINI_BATCH_SIZE = 32, \n",
    "    TOTAL_TIMESTEPS = 4096\n",
    "):\n",
    "    \n",
    "\n",
    "    if env is None:\n",
    "        raise ValueError(\"Please provide a valid environment instance.\")\n",
    "    elif model is None:\n",
    "        raise ValueError(\"Please provide a valid model instance.\")\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(\"Observation dim:\", obs_dim)\n",
    "    print(\"Action dim:\", act_dim)\n",
    "\n",
    "    global_step = 0\n",
    "    # episode_returns = []\n",
    "    # current_ep_return = 0.0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    while global_step < TOTAL_TIMESTEPS:\n",
    "        # 1) 采样一批 rollout\n",
    "        batch = collect_rollout(env, model, N_STEPS)\n",
    "        global_step += N_STEPS\n",
    "\n",
    "        obs_arr = batch[\"obs\"] \n",
    "        actions_arr = batch[\"actions\"]\n",
    "        old_logprobs_arr = batch[\"logprobs\"]\n",
    "        rewards_arr = batch[\"rewards\"]\n",
    "        dones_arr = batch[\"dones\"]\n",
    "        values_arr = batch[\"values\"]\n",
    "        next_value = batch[\"next_value\"]\n",
    "\n",
    "        # 2) 计算 GAE & returns\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards_arr, values_arr, \n",
    "            dones_arr, next_value,\n",
    "            gamma=GAMMA, lam=GAE_LAMBDA\n",
    "        )\n",
    "\n",
    "        # 标准化 advantage 可以加速训练\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # 转成 tensor\n",
    "        obs_t = torch.tensor(obs_arr, dtype=torch.float32)\n",
    "        actions_t = torch.tensor(actions_arr, dtype=torch.int64)\n",
    "        old_logprobs_t = torch.tensor(old_logprobs_arr, dtype=torch.float32)\n",
    "        advantages_t = torch.tensor(advantages, dtype=torch.float32)\n",
    "        returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # 3) PPO 多 epoch 训练\n",
    "        dataset_size = N_STEPS\n",
    "        indices = np.arange(dataset_size)\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, dataset_size, MINI_BATCH_SIZE):\n",
    "                end = start + MINI_BATCH_SIZE\n",
    "                mb_idx = indices[start:end]\n",
    "\n",
    "                mb_obs = obs_t[mb_idx]\n",
    "                mb_actions = actions_t[mb_idx]\n",
    "                mb_old_logprobs = old_logprobs_t[mb_idx]\n",
    "                mb_advantages = advantages_t[mb_idx]\n",
    "                mb_returns = returns_t[mb_idx]\n",
    "\n",
    "                # 计算当前策略下 log_prob, entropy, value\n",
    "                new_logprobs, entropy, values_pred = model.evaluate_actions(mb_obs, mb_actions)\n",
    "\n",
    "                # ratio = π_θ(a|s) / π_θ_old(a|s)\n",
    "                ratio = torch.exp(new_logprobs - mb_old_logprobs)\n",
    "\n",
    "                # PPO clipped surrogate\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # value function loss\n",
    "                critic_loss = nn.MSELoss()(values_pred, mb_returns)\n",
    "\n",
    "                # entropy 促进探索\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                loss = actor_loss + VF_COEF * critic_loss + ENT_COEF * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # 4) 打印训练信息（粗略统计一下最近一批的平均奖励）\n",
    "        batch_mean_return = rewards_arr.mean() * N_STEPS  # 很粗略，仅为参考\n",
    "        print(f\"[Step {global_step}] recent mean reward per step: {rewards_arr.mean():.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    torch.save(model.state_dict(), \"ppo_traffic_signal.pth\")\n",
    "    print(\"Training finished, model saved to ppo_traffic_signal.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchen/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:42: UserWarning: Call to deprecated function getCompleteRedYellowGreenDefinition, use getAllProgramLogics instead.\n",
      "  num_phases = len(self.sumo.trafficlight.getCompleteRedYellowGreenDefinition(self.ts_id)[0].phases)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 7ms, vehicles TOT 0 ACT 0 BUF 0)                      \n",
      "Environment setup complete. Ready for Model Initilizaiton...\n"
     ]
    }
   ],
   "source": [
    "from single_intersection import TrafficEnv\n",
    "\n",
    "# SUMO command (headless for speed)\n",
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# Initialize SUMO environment\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False                 # show SUMO GUI\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(\"Environment setup complete. Ready for Model Initilizaiton...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Starting training...\n"
     ]
    }
   ],
   "source": [
    "model_params  = {\n",
    "    \"obs_dim\": env.observation_space.shape[0],\n",
    "    \"act_dim\": env.action_space.n\n",
    "}\n",
    "\n",
    "ppo_model = ActorCritic(**model_params)\n",
    "print(\"Model initialized. Starting training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation dim: 16\n",
      "Action dim: 8\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 18288ms, vehicles TOT 0 ACT 0 BUF 0)                  \n",
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 4ms, vehicles TOT 0 ACT 0 BUF 0)                      \n",
      " Retrying in 1 seconds\n",
      "[Step 256] recent mean reward per step: -10.255\n",
      "Step #256.00 (0ms ?*RT. ?UPS, TraCI: 188ms, vehicles TOT 180 ACT 55 BUF 4)                \n",
      " Retrying in 1 seconds\n",
      "[Step 512] recent mean reward per step: -8.135\n",
      "Step #256.00 (0ms ?*RT. ?UPS, TraCI: 105ms, vehicles TOT 182 ACT 50 BUF 2)                \n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m train_params = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m:ppo_model,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m: env\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(model, env, GAMMA, GAE_LAMBDA, CLIP_EPS, LR, ENT_COEF, VF_COEF, MAX_GRAD_NORM, N_STEPS, N_EPOCHS, MINI_BATCH_SIZE, TOTAL_TIMESTEPS)\u001b[39m\n\u001b[32m     40\u001b[39m obs, info = env.reset()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m global_step < TOTAL_TIMESTEPS:\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# 1) 采样一批 rollout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     batch = \u001b[43mcollect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     global_step += N_STEPS\n\u001b[32m     47\u001b[39m     obs_arr = batch[\u001b[33m\"\u001b[39m\u001b[33mobs\u001b[39m\u001b[33m\"\u001b[39m] \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/ppo.py:135\u001b[39m, in \u001b[36mcollect_rollout\u001b[39m\u001b[34m(env, model, n_steps)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[32m    133\u001b[39m     action, log_prob, value = model.act(obs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     next_obs, reward, done, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# 这个 env 的 done 一直是 False，目前可以忽略 truncated，按持续任务处理\u001b[39;00m\n\u001b[32m    138\u001b[39m     obs_list.append(obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:111\u001b[39m, in \u001b[36mTrafficEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.sumo.simulationStep()\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Reward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Update history\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.prev_phase = action\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:149\u001b[39m, in \u001b[36mTrafficEnv._compute_reward\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m    148\u001b[39m     queue_reduction = \u001b[38;5;28mself\u001b[39m._compute_queue_reduction()\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     queue_abs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_queue_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     pressure = \u001b[38;5;28mself\u001b[39m._compute_pressure()\n\u001b[32m    151\u001b[39m     switch_penalty = \u001b[38;5;28mself\u001b[39m._compute_switch_penalty(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:195\u001b[39m, in \u001b[36mTrafficEnv._compute_queue_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m total_q = \u001b[32m0\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lane \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_lanes:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     vehs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msumo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetLastStepVehicleIDs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vehs:\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sumo.vehicle.getSpeed(v) < \u001b[32m0.1\u001b[39m:  \u001b[38;5;66;03m# True queue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/_lane.py:277\u001b[39m, in \u001b[36mLaneDomain.getLastStepVehicleIDs\u001b[39m\u001b[34m(self, laneID)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetLastStepVehicleIDs\u001b[39m(\u001b[38;5;28mself\u001b[39m, laneID):\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"getLastStepVehicleIDs(string) -> list(string)\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    Returns the ids of the vehicles for the last time step on the given lane.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLAST_STEP_VEHICLE_ID_LIST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaneID\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/domain.py:149\u001b[39m, in \u001b[36mDomain._getUniversal\u001b[39m\u001b[34m(self, varID, objectID, format, *values)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deprecatedFor:\n\u001b[32m    148\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m._name, \u001b[38;5;28mself\u001b[39m._deprecatedFor))\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m._retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/domain.py:154\u001b[39m, in \u001b[36mDomain._getCmd\u001b[39m\u001b[34m(self, varID, objID, format, *values)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[33m\"\u001b[39m\u001b[33mNot connected.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m r.readLength()\n\u001b[32m    156\u001b[39m response, retVarID = r.read(\u001b[33m\"\u001b[39m\u001b[33m!BB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/connection.py:131\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._socket.send(length + \u001b[38;5;28mself\u001b[39m._string)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreceiving\u001b[39m\u001b[33m\"\u001b[39m, result.getDebugString())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/connection.py:109\u001b[39m, in \u001b[36mConnection._recvExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m result = \u001b[38;5;28mbytes\u001b[39m()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) < \u001b[32m4\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    \"model\":ppo_model,\n",
    "    \"env\": env\n",
    "\n",
    "}\n",
    "\n",
    "train_ppo(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
