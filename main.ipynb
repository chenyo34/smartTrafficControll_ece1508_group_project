{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sumo_rl import SumoEnvironment\n",
    "import torch\n",
    "# Print the sumo environment path for further verification \n",
    "print(\"SUMO HOME:\", os.environ.get(\"SUMO_HOME\"))\n",
    "# SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def to_evaluate_agent(\n",
    "        env=None,\n",
    "        agent=\"heuristic\",\n",
    "        steps=1000,\n",
    "        phase_duration=10,\n",
    "        render=False,\n",
    "        seed=42,\n",
    "        to_save = None):\n",
    "    \"\"\" Evaluate the performance of a method in given SUMO env.\"\"\"\n",
    "\n",
    "    # Initialization  ->  file saving \n",
    "    sim_records = []\n",
    "    header = [\n",
    "        \"step\", \n",
    "        \"sim_time\",\n",
    "        \"avg_wait_time\", \n",
    "        # \"total_wait_time\",\n",
    "        \"queue_length\",\n",
    "        \"pressure\",\n",
    "        \"throughput\",\n",
    "        \"avg_speed\",\n",
    "        \"action\",\n",
    "        \"reward\"]\n",
    "    \n",
    "    # Initialization -> simulation loops\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    cur_phase, phase_timer, last_arrived = 0,0,0\n",
    "    done = False \n",
    "\n",
    "    for step in range(steps):\n",
    "        if render: env.render()\n",
    "\n",
    "        # debug \n",
    "        # print(\"Phase Time\" , phase_timer, \"Current Phase: \", cur_phase)\n",
    "\n",
    "        #################################\n",
    "        ###  Action Selection ###\n",
    "        #################################\n",
    "        if agent == \"heuristic\":\n",
    "            # Determine the action\n",
    "            if phase_timer >= phase_duration: # Pre-defined heuristic method\n",
    "                cur_phase = (cur_phase + 1) % env.action_space.n\n",
    "                phase_timer = 0\n",
    "            action = cur_phase\n",
    "            phase_timer += 1\n",
    "        elif agent == \"random\": # Random Method \n",
    "            action = env.action_space.sample()\n",
    "        # else: # Trained RL Agent \n",
    "        #     # action, _ \n",
    "\n",
    "        #################################\n",
    "        ###  Feed action and observe ###\n",
    "        #################################\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        sim_time = env.sumo.simulation.getTime()\n",
    "        # veh_ids = env.sumo.simulation.getIDList()\n",
    "        # avg_wait_time = info[\"avg_wait_time\"] # Not sure if it is available\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ###  Collect and store the metrics ###\n",
    "        #######################################\n",
    "        # wait_time_lst = []\n",
    "        # queue_length = 0\n",
    "        # lane_veh_counts = {}\n",
    "\n",
    "        # for veh in veh_ids:\n",
    "        #     cur_lane = env.sumo.vehicles.getLaneID(veh)\n",
    "        #     cur_speed = env.sumo.vehicles.getSpeed(veh)\n",
    "        #     cur_wait_time = env.sumo.vehicles.getWaitingTime(veh)\n",
    "\n",
    "        #     if cur_speed < 0.1:\n",
    "        #         queue_length += 1\n",
    "        #     lane_veh_counts[cur_lane] = lane_veh_counts.get(cur_lane, 0) + 1\n",
    "\n",
    "        #     # ====== Wait-Time ======\n",
    "        #     wait_time_lst.append(cur_wait_time)\n",
    "        #     # ========================\n",
    "\n",
    "        # # Pressure\n",
    "        # pressure = 0\n",
    "        # for lane, count in lane_veh_counts.items():\n",
    "        #     if 1:\n",
    "        #         # num_incoming += count\n",
    "        #         pressure += count\n",
    "        #     else:\n",
    "        #         num_outgoing += count\n",
    "        #         pressure -= count\n",
    "\n",
    "        # Avg wait and total wait\n",
    "        # avg_wait = np.mean(wait_time_lst) if wait_time_lst else 0\n",
    "        # total_wait = np.sum(wait_time_lst) if wait_time_lst else 0\n",
    "\n",
    "        # Avg speed and total speed\n",
    "        # avg_speed = np.mean(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "        # total_speed = np.sum(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "\n",
    "        # Throughput\n",
    "        # total_arrived = env.sumo.simulation.getArrivedNumber()\n",
    "        # throughput = total_arrived - last_arrived\n",
    "        # last_arrived = total_arrived\n",
    "\n",
    "        # Load the records into log\n",
    "        sim_records.append([\n",
    "            step,\n",
    "            sim_time,\n",
    "            info[\"waiting_time\"],\n",
    "            info[\"queue_length\"],\n",
    "            info[\"pressure\"],\n",
    "            info[\"throughput\"],\n",
    "            info[\"avg_speed\"],\n",
    "            action,\n",
    "            reward])\n",
    "\n",
    "        if done:\n",
    "            obs, info = env.reset(seed=seed)\n",
    "            last_arrived = 0\n",
    "        \n",
    "    env.close()\n",
    "\n",
    "    if to_save:\n",
    "        # supposed: results/***_evaluation_records.csv \n",
    "        folder = \"results\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(folder, f\"{to_save}_evaluation_records.csv\")\n",
    "\n",
    "        with open(save_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(sim_records)\n",
    "        \n",
    "        print(f\"Evaluation records saved to: {save_path}\")\n",
    "\n",
    "        \n",
    "        return sim_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchen/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:48: UserWarning: Call to deprecated function getCompleteRedYellowGreenDefinition, use getAllProgramLogics instead.\n",
      "  num_phases = len(self.sumo.trafficlight.getCompleteRedYellowGreenDefinition(self.ts_id)[0].phases)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 47ms, vehicles TOT 0 ACT 0 BUF 0)                     \n",
      "Step #100.00 (0ms ?*RT. ?UPS, TraCI: 17ms, vehicles TOT 76 ACT 52 BUF 0)                   \n",
      "Evaluation records saved to: results/test_evaluation_records.csv\n"
     ]
    }
   ],
   "source": [
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# import traci\n",
    "# traci.close(False)\n",
    "\n",
    "# Initialize SUMO environment\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "logs = to_evaluate_agent(\n",
    "    env=env,\n",
    "    agent=\"heuristic\",\n",
    "    steps=100,\n",
    "    to_save=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the training function\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "# from torch import nn\n",
    "# from helper_func import plot_traff_metrics\n",
    "\n",
    "# Integrate into train.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we build the different environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "from train import train_ppo\n",
    "import torch.optim as optim\n",
    "from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "from torch import nn\n",
    "from helper_func import plot_traff_metrics\n",
    "\n",
    "\n",
    "# SUMO command (headless for speed)\n",
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# Environment with noise \n",
    "env_w_noise = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False,\n",
    "    noise=True,\n",
    "    noise_sigma=1.0\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete.\")\n",
    "\n",
    "# Basic Envivronment without noise (default)\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments across all combinations:\n",
    "# 1. Noise v.s. Without Noise\n",
    "# 2. Heuristic v.s. RL Agent\n",
    "# 3. Different Reward Function \n",
    "\n",
    "from run_experiments import run_experiments\n",
    "\n",
    "# SUMO command configuration\n",
    "sumo_cmd = [\n",
    "    \"--no-warnings\", \"true\",\n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"\n",
    "\n",
    "# Run all experiments\n",
    "# This will iterate through all combinations:\n",
    "# - Noise: True/False (2 options)\n",
    "# - Agent: \"heuristic\"/\"rl\" (2 options)  \n",
    "# - Reward configs: 4 different configurations\n",
    "# Total: 2 x 2 x 4 = 16 experiments\n",
    "\n",
    "results = run_experiments(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    eval_steps=1000,      # Evaluation steps per experiment\n",
    "    train_timesteps=4096, # Training timesteps for RL agents (only used for RL experiments)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal experiments completed: {len(results)}\")\n",
    "print(\"Check 'results/experiments_summary.csv' for summary of all experiments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params  = {\n",
    "#     \"obs_dim\": env.observation_space.shape[0],\n",
    "#     \"act_dim\": env.action_space.n\n",
    "# }\n",
    "\n",
    "# ppo_model = ActorCritic(**model_params)\n",
    "# print(\"Model initialized. Starting training...\")\n",
    "\n",
    "# train_params = {\n",
    "#     \"model\": ppo_model,\n",
    "#     \"env\": env\n",
    "# }\n",
    "\n",
    "# model_hist = train_ppo(**train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL agent and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
