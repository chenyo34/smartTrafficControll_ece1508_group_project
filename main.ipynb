{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sumo_rl import SumoEnvironment\n",
    "import torch\n",
    "# Print the sumo environment path for further verification \n",
    "print(\"SUMO HOME:\", os.environ.get(\"SUMO_HOME\"))\n",
    "# SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def to_evaluate_agent(\n",
    "        env=None,\n",
    "        agent=\"heuristic\",\n",
    "        steps=1000,\n",
    "        phase_duration=10,\n",
    "        render=False,\n",
    "        seed=42,\n",
    "        to_save = None):\n",
    "    \"\"\" Evaluate the performance of a method in given SUMO env.\"\"\"\n",
    "\n",
    "    # Initialization  ->  file saving \n",
    "    sim_records = []\n",
    "    header = [\n",
    "        \"step\", \n",
    "        \"sim_time\",\n",
    "        \"avg_wait_time\", \n",
    "        # \"total_wait_time\",\n",
    "        \"queue_length\",\n",
    "        \"pressure\",\n",
    "        \"throughput\",\n",
    "        \"avg_speed\",\n",
    "        \"action\",\n",
    "        \"reward\"]\n",
    "    \n",
    "    # Initialization -> simulation loops\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    cur_phase, phase_timer, last_arrived = 0,0,0\n",
    "    done = False \n",
    "\n",
    "    for step in range(steps):\n",
    "        if render: env.render()\n",
    "\n",
    "        # debug \n",
    "        # print(\"Phase Time\" , phase_timer, \"Current Phase: \", cur_phase)\n",
    "\n",
    "        #################################\n",
    "        ###  Action Selection ###\n",
    "        #################################\n",
    "        if agent == \"heuristic\":\n",
    "            # Determine the action\n",
    "            if phase_timer >= phase_duration: # Pre-defined heuristic method\n",
    "                cur_phase = (cur_phase + 1) % env.action_space.n\n",
    "                phase_timer = 0\n",
    "            action = cur_phase\n",
    "            phase_timer += 1\n",
    "        elif agent == \"random\": # Random Method \n",
    "            action = env.action_space.sample()\n",
    "        # else: # Trained RL Agent \n",
    "        #     # action, _ \n",
    "\n",
    "        #################################\n",
    "        ###  Feed action and observe ###\n",
    "        #################################\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        sim_time = env.sumo.simulation.getTime()\n",
    "        # veh_ids = env.sumo.simulation.getIDList()\n",
    "        # avg_wait_time = info[\"avg_wait_time\"] # Not sure if it is available\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ###  Collect and store the metrics ###\n",
    "        #######################################\n",
    "        # wait_time_lst = []\n",
    "        # queue_length = 0\n",
    "        # lane_veh_counts = {}\n",
    "\n",
    "        # for veh in veh_ids:\n",
    "        #     cur_lane = env.sumo.vehicles.getLaneID(veh)\n",
    "        #     cur_speed = env.sumo.vehicles.getSpeed(veh)\n",
    "        #     cur_wait_time = env.sumo.vehicles.getWaitingTime(veh)\n",
    "\n",
    "        #     if cur_speed < 0.1:\n",
    "        #         queue_length += 1\n",
    "        #     lane_veh_counts[cur_lane] = lane_veh_counts.get(cur_lane, 0) + 1\n",
    "\n",
    "        #     # ====== Wait-Time ======\n",
    "        #     wait_time_lst.append(cur_wait_time)\n",
    "        #     # ========================\n",
    "\n",
    "        # # Pressure\n",
    "        # pressure = 0\n",
    "        # for lane, count in lane_veh_counts.items():\n",
    "        #     if 1:\n",
    "        #         # num_incoming += count\n",
    "        #         pressure += count\n",
    "        #     else:\n",
    "        #         num_outgoing += count\n",
    "        #         pressure -= count\n",
    "\n",
    "        # Avg wait and total wait\n",
    "        # avg_wait = np.mean(wait_time_lst) if wait_time_lst else 0\n",
    "        # total_wait = np.sum(wait_time_lst) if wait_time_lst else 0\n",
    "\n",
    "        # Avg speed and total speed\n",
    "        # avg_speed = np.mean(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "        # total_speed = np.sum(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "\n",
    "        # Throughput\n",
    "        # total_arrived = env.sumo.simulation.getArrivedNumber()\n",
    "        # throughput = total_arrived - last_arrived\n",
    "        # last_arrived = total_arrived\n",
    "\n",
    "        # Load the records into log\n",
    "        sim_records.append([\n",
    "            step,\n",
    "            sim_time,\n",
    "            info[\"waiting_time\"],\n",
    "            info[\"queue_length\"],\n",
    "            info[\"pressure\"],\n",
    "            info[\"throughput\"],\n",
    "            info[\"avg_speed\"],\n",
    "            action,\n",
    "            reward])\n",
    "\n",
    "        if done:\n",
    "            obs, info = env.reset(seed=seed)\n",
    "            last_arrived = 0\n",
    "        \n",
    "    env.close()\n",
    "\n",
    "    if to_save:\n",
    "        # supposed: results/***_evaluation_records.csv \n",
    "        folder = \"results\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(folder, f\"{to_save}_evaluation_records.csv\")\n",
    "\n",
    "        with open(save_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(sim_records)\n",
    "        \n",
    "        print(f\"Evaluation records saved to: {save_path}\")\n",
    "\n",
    "        \n",
    "        return sim_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# import traci\n",
    "# traci.close(False)\n",
    "\n",
    "# Initialize SUMO environment\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "logs = to_evaluate_agent(\n",
    "    env=env,\n",
    "    agent=\"heuristic\",\n",
    "    steps=100,\n",
    "    to_save=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the training function\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "# from torch import nn\n",
    "# from helper_func import plot_traff_metrics\n",
    "\n",
    "# Integrate into train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we build the different environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "from train import train_ppo\n",
    "import torch.optim as optim\n",
    "from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "from torch import nn\n",
    "from helper_func import plot_traff_metrics\n",
    "\n",
    "\n",
    "# SUMO command (headless for speed)\n",
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# Environment with noise \n",
    "env_w_noise = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False,\n",
    "    noise=True,\n",
    "    noise_sigma=1.0\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete.\")\n",
    "\n",
    "# Basic Envivronment without noise (default)\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: Finding best reward function and noise combination (RL agent)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Stage 1 - Experiment 1: Noise=True, Reward=default\n",
      "================================================================================\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchen/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:59: UserWarning: Call to deprecated function getCompleteRedYellowGreenDefinition, use getAllProgramLogics instead.\n",
      "  num_phases = len(self.sumo.trafficlight.getCompleteRedYellowGreenDefinition(self.ts_id)[0].phases)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RL agent...\n",
      " Retrying in 1 seconds\n",
      "Debug: Reached 0 steps\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 256/16384 (1.6%) | Reward: -29.495 | Waiting time: 0.157 | Queue length: 1.461 | Throughput: 0.056\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 512/16384 (3.1%) | Reward: -24.752 | Waiting time: 0.203 | Queue length: 1.253 | Throughput: 0.059\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 768/16384 (4.7%) | Reward: -31.635 | Waiting time: 0.217 | Queue length: 1.583 | Throughput: 0.066\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1024/16384 (6.2%) | Reward: -35.159 | Waiting time: 0.308 | Queue length: 1.864 | Throughput: 0.047\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1280/16384 (7.8%) | Reward: -32.408 | Waiting time: 0.217 | Queue length: 1.605 | Throughput: 0.059\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1536/16384 (9.4%) | Reward: -30.720 | Waiting time: 0.243 | Queue length: 1.631 | Throughput: 0.054\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1792/16384 (10.9%) | Reward: -30.998 | Waiting time: 0.235 | Queue length: 1.660 | Throughput: 0.054\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2048/16384 (12.5%) | Reward: -35.647 | Waiting time: 0.292 | Queue length: 1.951 | Throughput: 0.052\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2304/16384 (14.1%) | Reward: -31.574 | Waiting time: 0.259 | Queue length: 1.762 | Throughput: 0.052\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2560/16384 (15.6%) | Reward: -28.331 | Waiting time: 0.336 | Queue length: 1.609 | Throughput: 0.075\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2816/16384 (17.2%) | Reward: -29.545 | Waiting time: 0.231 | Queue length: 1.621 | Throughput: 0.065\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3072/16384 (18.8%) | Reward: -29.251 | Waiting time: 0.423 | Queue length: 1.877 | Throughput: 0.058\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3328/16384 (20.3%) | Reward: -34.546 | Waiting time: 0.257 | Queue length: 1.823 | Throughput: 0.056\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3584/16384 (21.9%) | Reward: -28.693 | Waiting time: 0.302 | Queue length: 1.503 | Throughput: 0.061\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3840/16384 (23.4%) | Reward: -31.263 | Waiting time: 0.252 | Queue length: 1.680 | Throughput: 0.054\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4096/16384 (25.0%) | Reward: -29.838 | Waiting time: 0.214 | Queue length: 1.615 | Throughput: 0.064\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4352/16384 (26.6%) | Reward: -24.559 | Waiting time: 0.447 | Queue length: 1.826 | Throughput: 0.061\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4608/16384 (28.1%) | Reward: -30.216 | Waiting time: 0.336 | Queue length: 1.826 | Throughput: 0.059\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4864/16384 (29.7%) | Reward: -24.040 | Waiting time: 0.404 | Queue length: 1.676 | Throughput: 0.060\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5120/16384 (31.2%) | Reward: -27.976 | Waiting time: 0.430 | Queue length: 2.007 | Throughput: 0.063\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5376/16384 (32.8%) | Reward: -26.756 | Waiting time: 0.453 | Queue length: 1.891 | Throughput: 0.054\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5632/16384 (34.4%) | Reward: -30.682 | Waiting time: 0.425 | Queue length: 1.750 | Throughput: 0.061\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5888/16384 (35.9%) | Reward: -32.340 | Waiting time: 0.334 | Queue length: 1.812 | Throughput: 0.056\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6144/16384 (37.5%) | Reward: -26.935 | Waiting time: 0.524 | Queue length: 1.863 | Throughput: 0.063\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6400/16384 (39.1%) | Reward: -36.270 | Waiting time: 0.337 | Queue length: 2.111 | Throughput: 0.058\n",
      "Debug: Reached 6400 steps\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6656/16384 (40.6%) | Reward: -37.004 | Waiting time: 0.320 | Queue length: 2.017 | Throughput: 0.057\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6912/16384 (42.2%) | Reward: -35.059 | Waiting time: 0.382 | Queue length: 1.970 | Throughput: 0.057\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7168/16384 (43.8%) | Reward: -36.703 | Waiting time: 0.374 | Queue length: 2.113 | Throughput: 0.053\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7424/16384 (45.3%) | Reward: -32.105 | Waiting time: 0.296 | Queue length: 1.948 | Throughput: 0.056\n",
      " Retrying in 1 seconds\n",
      "Interrupt signal received, trying to exit gracefully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     37\u001b[39m noise_options = [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[32m     39\u001b[39m train_model_configs = {\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLR\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3e-4\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mN_STEPS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m256\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTOTAL_TIMESTEPS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4096\u001b[39m*\u001b[32m4\u001b[39m\n\u001b[32m     45\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m results = \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43msumo_cmd\u001b[49m\u001b[43m=\u001b[49m\u001b[43msumo_cmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTLS_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_model_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Evaluation steps per experiment\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_timesteps=50000, # Training timesteps for RL agents (only used for RL experiments)\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal experiments completed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCheck \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresults/experiments_summary.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for summary of all experiments\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/run_experiments.py:294\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m(sumo_cmd, tls_id, reward_configs, noise_options, eval_steps, seed, metric_for_selection, train_kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_kwargs:\n\u001b[32m    292\u001b[39m     base_train_kwargs.update(train_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbase_train_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m model.eval()\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Create a new environment for evaluation since training closed the previous one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/train.py:64\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(model, env, GAMMA, GAE_LAMBDA, CLIP_EPS, LR, ENT_COEF, VF_COEF, MAX_GRAD_NORM, N_STEPS, N_EPOCHS, MINI_BATCH_SIZE, TOTAL_TIMESTEPS, close_env, save_model, model_save_path)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDebug: Reached \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m steps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 1) Global Step: Roll-out sampling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m batch = \u001b[43mcollect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m global_step += N_STEPS\n\u001b[32m     67\u001b[39m obs_arr = batch[\u001b[33m\"\u001b[39m\u001b[33mobs\u001b[39m\u001b[33m\"\u001b[39m] \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/ppo.py:112\u001b[39m, in \u001b[36mcollect_rollout\u001b[39m\u001b[34m(env, model, n_steps)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[32m    110\u001b[39m     action, log_prob, value = model.act(obs)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     next_obs, reward, done, truncated, info= \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# 这个 env 的 done 一直是 False，目前可以忽略 truncated，按持续任务处理\u001b[39;00m\n\u001b[32m    115\u001b[39m     obs_list.append(obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:261\u001b[39m, in \u001b[36mTrafficEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    258\u001b[39m     truncated = \u001b[38;5;28mself\u001b[39m.steps >= \u001b[38;5;28mself\u001b[39m.max_steps \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax_steps\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Evaluation Metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m avg_speed=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_computer_avg_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m throughput=\u001b[38;5;28mself\u001b[39m._compute_throughput()\n\u001b[32m    263\u001b[39m waiting_time=\u001b[38;5;28mself\u001b[39m._compute_avg_waiting_time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:403\u001b[39m, in \u001b[36mTrafficEnv._computer_avg_speed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m vehs_speeds=[]\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lane \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_lanes:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     vehs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msumo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetLastStepVehicleIDs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vehs:\n\u001b[32m    405\u001b[39m         v_speed=\u001b[38;5;28mself\u001b[39m.sumo.vehicle.getSpeed(v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/_lane.py:277\u001b[39m, in \u001b[36mLaneDomain.getLastStepVehicleIDs\u001b[39m\u001b[34m(self, laneID)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetLastStepVehicleIDs\u001b[39m(\u001b[38;5;28mself\u001b[39m, laneID):\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"getLastStepVehicleIDs(string) -> list(string)\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    Returns the ids of the vehicles for the last time step on the given lane.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLAST_STEP_VEHICLE_ID_LIST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaneID\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/domain.py:149\u001b[39m, in \u001b[36mDomain._getUniversal\u001b[39m\u001b[34m(self, varID, objectID, format, *values)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deprecatedFor:\n\u001b[32m    148\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m._name, \u001b[38;5;28mself\u001b[39m._deprecatedFor))\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m._retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/domain.py:154\u001b[39m, in \u001b[36mDomain._getCmd\u001b[39m\u001b[34m(self, varID, objID, format, *values)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[33m\"\u001b[39m\u001b[33mNot connected.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m r.readLength()\n\u001b[32m    156\u001b[39m response, retVarID = r.read(\u001b[33m\"\u001b[39m\u001b[33m!BB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/sumo/tools/traci/connection.py:130\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m result = \u001b[38;5;28mself\u001b[39m._recvExact()\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/miniconda3/bin/sumo\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31msumo\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/miniconda3/lib/python3.13/site-packages/sumo/__init__.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<lambda>\u001b[0m\n",
      "    return lambda: sys.exit(\u001b[31msubprocess.call\u001b[0m\u001b[1;31m([os.path.join(SUMO_HOME, 'bin', app)] + sys.argv[1:], env=os.environ)\u001b[0m)\n",
      "                            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/miniconda3/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m397\u001b[0m, in \u001b[35mcall\u001b[0m\n",
      "    return \u001b[31mp.wait\u001b[0m\u001b[1;31m(timeout=timeout)\u001b[0m\n",
      "           \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/miniconda3/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m1280\u001b[0m, in \u001b[35mwait\u001b[0m\n",
      "    return \u001b[31mself._wait\u001b[0m\u001b[1;31m(timeout=timeout)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/miniconda3/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m2066\u001b[0m, in \u001b[35m_wait\u001b[0m\n",
      "    (pid, sts) = \u001b[31mself._try_wait\u001b[0m\u001b[1;31m(0)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/miniconda3/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m2024\u001b[0m, in \u001b[35m_try_wait\u001b[0m\n",
      "    (pid, sts) = \u001b[31mos.waitpid\u001b[0m\u001b[1;31m(self.pid, wait_flags)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run experiments across all combinations:\n",
    "# 1. Noise v.s. Without Noise\n",
    "# 2. Heuristic v.s. RL Agent\n",
    "# 3. Different Reward Function \n",
    "\n",
    "from run_experiments import run_experiments\n",
    "\n",
    "# SUMO command configuration\n",
    "sumo_cmd = [\n",
    "    \"--no-warnings\", \"true\",\n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"\n",
    "\n",
    "# Run all experiments\n",
    "# This will iterate through all combinations:\n",
    "# - Noise: True/False (2 options)\n",
    "# - Agent: \"heuristic\"/\"rl\" (2 options)  \n",
    "# - Reward configs: 4 different configurations\n",
    "# Total: 2 x 2 x 4 = 16 experiments\n",
    "\n",
    "# queue_reduction， queue_abs， pressure， switch_penalty， throughput\n",
    "# reward_configs = [\n",
    "#     (\"default\", 1.0, 0.3, 0.15, 0.01, 0.005),\n",
    "#     (\"queue_focused\", 2.0, 0.5, 0.1, 0.05, 0.01),\n",
    "#     (\"pressure_focused\", 1.0, 0.2, 0.3, 0.05, 0.01),\n",
    "#     (\"throughput_focused\", 0.5, 0.2, 0.1, 0.05, 0.02),\n",
    "# ]\n",
    "\n",
    "reward_configs = [\n",
    "    (\"default\", 1.0, 0.3, 0.15, 0.05, 0.005)\n",
    "]\n",
    "\n",
    "noise_options = [True, False]\n",
    "\n",
    "train_model_configs = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"N_STEPS\": 256,\n",
    "    \"N_EPOCHS\": 4,\n",
    "    \"MINI_BATCH_SIZE\": 64,\n",
    "    \"TOTAL_TIMESTEPS\": 4096*4\n",
    "}\n",
    "\n",
    "results = run_experiments(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    reward_configs=reward_configs,\n",
    "    noise_options=noise_options,\n",
    "    train_kwargs=train_model_configs,\n",
    "    eval_steps=1000,      # Evaluation steps per experiment\n",
    "    # train_timesteps=50000, # Training timesteps for RL agents (only used for RL experiments)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nTotal experiments completed: {len(results)}\")\n",
    "print(\"Check 'results/experiments_summary.csv' for summary of all experiments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params  = {\n",
    "#     \"obs_dim\": env.observation_space.shape[0],\n",
    "#     \"act_dim\": env.action_space.n\n",
    "# }\n",
    "\n",
    "# ppo_model = ActorCritic(**model_params)\n",
    "# print(\"Model initialized. Starting training...\")\n",
    "\n",
    "# train_params = {\n",
    "#     \"model\": ppo_model,\n",
    "#     \"env\": env\n",
    "# }\n",
    "\n",
    "# model_hist = train_ppo(**train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
