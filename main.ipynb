{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sumo_rl import SumoEnvironment\n",
    "import torch\n",
    "# Print the sumo environment path for further verification \n",
    "print(\"SUMO HOME:\", os.environ.get(\"SUMO_HOME\"))\n",
    "# SUMO HOME: /opt/miniconda3/lib/python3.13/site-packages/sumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def to_evaluate_agent(\n",
    "        env=None,\n",
    "        agent=\"heuristic\",\n",
    "        steps=1000,\n",
    "        phase_duration=10,\n",
    "        render=False,\n",
    "        seed=42,\n",
    "        to_save = None):\n",
    "    \"\"\" Evaluate the performance of a method in given SUMO env.\"\"\"\n",
    "\n",
    "    # Initialization  ->  file saving \n",
    "    sim_records = []\n",
    "    header = [\n",
    "        \"step\", \n",
    "        \"sim_time\",\n",
    "        \"avg_wait_time\", \n",
    "        # \"total_wait_time\",\n",
    "        \"queue_length\",\n",
    "        \"pressure\",\n",
    "        \"throughput\",\n",
    "        \"avg_speed\",\n",
    "        \"action\",\n",
    "        \"reward\"]\n",
    "    \n",
    "    # Initialization -> simulation loops\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    cur_phase, phase_timer, last_arrived = 0,0,0\n",
    "    done = False \n",
    "\n",
    "    for step in range(steps):\n",
    "        if render: env.render()\n",
    "\n",
    "        # debug \n",
    "        # print(\"Phase Time\" , phase_timer, \"Current Phase: \", cur_phase)\n",
    "\n",
    "        #################################\n",
    "        ###  Action Selection ###\n",
    "        #################################\n",
    "        if agent == \"heuristic\":\n",
    "            # Determine the action\n",
    "            if phase_timer >= phase_duration: # Pre-defined heuristic method\n",
    "                cur_phase = (cur_phase + 1) % env.action_space.n\n",
    "                phase_timer = 0\n",
    "            action = cur_phase\n",
    "            phase_timer += 1\n",
    "        elif agent == \"random\": # Random Method \n",
    "            action = env.action_space.sample()\n",
    "        # else: # Trained RL Agent \n",
    "        #     # action, _ \n",
    "\n",
    "        #################################\n",
    "        ###  Feed action and observe ###\n",
    "        #################################\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        sim_time = env.sumo.simulation.getTime()\n",
    "        # veh_ids = env.sumo.simulation.getIDList()\n",
    "        # avg_wait_time = info[\"avg_wait_time\"] # Not sure if it is available\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ###  Collect and store the metrics ###\n",
    "        #######################################\n",
    "        # wait_time_lst = []\n",
    "        # queue_length = 0\n",
    "        # lane_veh_counts = {}\n",
    "\n",
    "        # for veh in veh_ids:\n",
    "        #     cur_lane = env.sumo.vehicles.getLaneID(veh)\n",
    "        #     cur_speed = env.sumo.vehicles.getSpeed(veh)\n",
    "        #     cur_wait_time = env.sumo.vehicles.getWaitingTime(veh)\n",
    "\n",
    "        #     if cur_speed < 0.1:\n",
    "        #         queue_length += 1\n",
    "        #     lane_veh_counts[cur_lane] = lane_veh_counts.get(cur_lane, 0) + 1\n",
    "\n",
    "        #     # ====== Wait-Time ======\n",
    "        #     wait_time_lst.append(cur_wait_time)\n",
    "        #     # ========================\n",
    "\n",
    "        # # Pressure\n",
    "        # pressure = 0\n",
    "        # for lane, count in lane_veh_counts.items():\n",
    "        #     if 1:\n",
    "        #         # num_incoming += count\n",
    "        #         pressure += count\n",
    "        #     else:\n",
    "        #         num_outgoing += count\n",
    "        #         pressure -= count\n",
    "\n",
    "        # Avg wait and total wait\n",
    "        # avg_wait = np.mean(wait_time_lst) if wait_time_lst else 0\n",
    "        # total_wait = np.sum(wait_time_lst) if wait_time_lst else 0\n",
    "\n",
    "        # Avg speed and total speed\n",
    "        # avg_speed = np.mean(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "        # total_speed = np.sum(env.sumo.vehicles.getSpeed(vid) for vid in veh_ids)\n",
    "\n",
    "        # Throughput\n",
    "        # total_arrived = env.sumo.simulation.getArrivedNumber()\n",
    "        # throughput = total_arrived - last_arrived\n",
    "        # last_arrived = total_arrived\n",
    "\n",
    "        # Load the records into log\n",
    "        sim_records.append([\n",
    "            step,\n",
    "            sim_time,\n",
    "            info[\"waiting_time\"],\n",
    "            info[\"queue_length\"],\n",
    "            info[\"pressure\"],\n",
    "            info[\"throughput\"],\n",
    "            info[\"avg_speed\"],\n",
    "            action,\n",
    "            reward])\n",
    "\n",
    "        if done:\n",
    "            obs, info = env.reset(seed=seed)\n",
    "            last_arrived = 0\n",
    "        \n",
    "    env.close()\n",
    "\n",
    "    if to_save:\n",
    "        # supposed: results/***_evaluation_records.csv \n",
    "        folder = \"results\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(folder, f\"{to_save}_evaluation_records.csv\")\n",
    "\n",
    "        with open(save_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(sim_records)\n",
    "        \n",
    "        print(f\"Evaluation records saved to: {save_path}\")\n",
    "\n",
    "        \n",
    "        return sim_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# import traci\n",
    "# traci.close(False)\n",
    "\n",
    "# Initialize SUMO environment\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "logs = to_evaluate_agent(\n",
    "    env=env,\n",
    "    agent=\"heuristic\",\n",
    "    steps=100,\n",
    "    to_save=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the training function\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "# from torch import nn\n",
    "# from helper_func import plot_traff_metrics\n",
    "\n",
    "# Integrate into train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we build the different environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages and libs.\n",
    "from single_intersection import TrafficEnv\n",
    "from train import train_ppo\n",
    "import torch.optim as optim\n",
    "from ppo import ActorCritic, compute_gae, collect_rollout\n",
    "from torch import nn\n",
    "from helper_func import plot_traff_metrics\n",
    "\n",
    "\n",
    "# SUMO command (headless for speed)\n",
    "sumo_cmd = [\n",
    "    # \"--start\", # Uncomment this line while using the GUI for visualization \n",
    "    \"--no-warnings\", \"true\", # Uncomment this line to \n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"    \n",
    "\n",
    "# Environment with noise \n",
    "env_w_noise = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False,\n",
    "    noise=True,\n",
    "    noise_sigma=1.0\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete.\")\n",
    "\n",
    "# Basic Envivronment without noise (default)\n",
    "env = TrafficEnv(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    gui=False   # show SUMO GUI\n",
    ")\n",
    "\n",
    "# Check if the environment is working \n",
    "obs, info = env.reset()\n",
    "print(f\"Environment {env.__class__.__name__} setup complete. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: Finding best reward function and noise combination (RL agent)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Stage 1 - Experiment 1: Noise=True, Reward=queue_focused\n",
      "================================================================================\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchen/Documents/MIE/ECE1508F3/smartTrafficControll_ece1508_group_project/single_intersection.py:59: UserWarning: Call to deprecated function getCompleteRedYellowGreenDefinition, use getAllProgramLogics instead.\n",
      "  num_phases = len(self.sumo.trafficlight.getCompleteRedYellowGreenDefinition(self.ts_id)[0].phases)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RL agent...\n",
      " Retrying in 1 seconds\n",
      "Debug: Reached 0 steps\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 256/10000 (2.6%) | Reward: -15.588 | Waiting time: 0.638 | Queue length: 0.327 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 512/10000 (5.1%) | Reward: -17.318 | Waiting time: 0.589 | Queue length: 0.323 | Throughput: 0.551\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 768/10000 (7.7%) | Reward: -17.371 | Waiting time: 0.610 | Queue length: 0.346 | Throughput: 0.547\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1024/10000 (10.2%) | Reward: -14.867 | Waiting time: 0.630 | Queue length: 0.306 | Throughput: 0.549\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1280/10000 (12.8%) | Reward: -20.045 | Waiting time: 0.595 | Queue length: 0.377 | Throughput: 0.546\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1536/10000 (15.4%) | Reward: -17.706 | Waiting time: 0.633 | Queue length: 0.345 | Throughput: 0.545\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 1792/10000 (17.9%) | Reward: -20.332 | Waiting time: 0.617 | Queue length: 0.411 | Throughput: 0.546\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2048/10000 (20.5%) | Reward: -19.869 | Waiting time: 0.592 | Queue length: 0.381 | Throughput: 0.547\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2304/10000 (23.0%) | Reward: -18.560 | Waiting time: 0.592 | Queue length: 0.351 | Throughput: 0.550\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2560/10000 (25.6%) | Reward: -17.049 | Waiting time: 0.623 | Queue length: 0.346 | Throughput: 0.542\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 2816/10000 (28.2%) | Reward: -18.308 | Waiting time: 0.616 | Queue length: 0.373 | Throughput: 0.539\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3072/10000 (30.7%) | Reward: -16.267 | Waiting time: 0.680 | Queue length: 0.387 | Throughput: 0.552\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3328/10000 (33.3%) | Reward: -14.976 | Waiting time: 0.598 | Queue length: 0.298 | Throughput: 0.545\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3584/10000 (35.8%) | Reward: -16.227 | Waiting time: 0.622 | Queue length: 0.332 | Throughput: 0.546\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 3840/10000 (38.4%) | Reward: -14.989 | Waiting time: 0.625 | Queue length: 0.315 | Throughput: 0.550\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4096/10000 (41.0%) | Reward: -16.871 | Waiting time: 0.629 | Queue length: 0.350 | Throughput: 0.541\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4352/10000 (43.5%) | Reward: -15.543 | Waiting time: 0.596 | Queue length: 0.307 | Throughput: 0.558\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4608/10000 (46.1%) | Reward: -15.430 | Waiting time: 0.591 | Queue length: 0.283 | Throughput: 0.556\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 4864/10000 (48.6%) | Reward: -17.987 | Waiting time: 0.617 | Queue length: 0.358 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5120/10000 (51.2%) | Reward: -16.411 | Waiting time: 0.576 | Queue length: 0.312 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5376/10000 (53.8%) | Reward: -21.126 | Waiting time: 0.659 | Queue length: 0.436 | Throughput: 0.547\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5632/10000 (56.3%) | Reward: -20.109 | Waiting time: 0.614 | Queue length: 0.400 | Throughput: 0.549\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 5888/10000 (58.9%) | Reward: -17.773 | Waiting time: 0.644 | Queue length: 0.361 | Throughput: 0.543\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6144/10000 (61.4%) | Reward: -19.260 | Waiting time: 0.621 | Queue length: 0.362 | Throughput: 0.548\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6400/10000 (64.0%) | Reward: -17.645 | Waiting time: 0.608 | Queue length: 0.352 | Throughput: 0.544\n",
      "Debug: Reached 6400 steps\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6656/10000 (66.6%) | Reward: -15.616 | Waiting time: 0.603 | Queue length: 0.317 | Throughput: 0.548\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 6912/10000 (69.1%) | Reward: -19.178 | Waiting time: 0.626 | Queue length: 0.391 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7168/10000 (71.7%) | Reward: -20.938 | Waiting time: 0.619 | Queue length: 0.405 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7424/10000 (74.2%) | Reward: -17.368 | Waiting time: 0.608 | Queue length: 0.331 | Throughput: 0.544\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7680/10000 (76.8%) | Reward: -18.222 | Waiting time: 0.597 | Queue length: 0.346 | Throughput: 0.543\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 7936/10000 (79.4%) | Reward: -14.157 | Waiting time: 0.637 | Queue length: 0.311 | Throughput: 0.545\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 8192/10000 (81.9%) | Reward: -16.402 | Waiting time: 0.615 | Queue length: 0.336 | Throughput: 0.543\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 8448/10000 (84.5%) | Reward: -17.286 | Waiting time: 0.607 | Queue length: 0.356 | Throughput: 0.540\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 8704/10000 (87.0%) | Reward: -15.747 | Waiting time: 0.603 | Queue length: 0.298 | Throughput: 0.541\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 8960/10000 (89.6%) | Reward: -16.005 | Waiting time: 0.621 | Queue length: 0.325 | Throughput: 0.546\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 9216/10000 (92.2%) | Reward: -19.171 | Waiting time: 0.616 | Queue length: 0.373 | Throughput: 0.548\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 9472/10000 (94.7%) | Reward: -21.372 | Waiting time: 0.622 | Queue length: 0.416 | Throughput: 0.547\n",
      " Retrying in 1 seconds\n",
      "[Training] Step 9728/10000 (97.3%) | Reward: -17.674 | Waiting time: 0.648 | Queue length: 0.381 | Throughput: 0.543\n",
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run experiments across all combinations:\n",
    "# 1. Noise v.s. Without Noise\n",
    "# 2. Heuristic v.s. RL Agent\n",
    "# 3. Different Reward Function \n",
    "\n",
    "from run_experiments import run_experiments\n",
    "import traci\n",
    "\n",
    "try:\n",
    "    if traci.isLoaded():\n",
    "        traci.close(False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# SUMO command configuration\n",
    "sumo_cmd = [\n",
    "    \"--no-warnings\", \"true\",\n",
    "    \"-n\", \"single-intersection.net.xml\",\n",
    "    \"-r\", \"single-intersection-vertical.rou.xml\",\n",
    "    \"--step-length\", \"1.0\"\n",
    "]\n",
    "\n",
    "TLS_ID = \"t\"\n",
    "\n",
    "# Run all experiments\n",
    "# This will iterate through all combinations:\n",
    "# - Noise: True/False (2 options)\n",
    "# - Agent: \"heuristic\"/\"rl\" (2 options)  \n",
    "# - Reward configs: 4 different configurations\n",
    "# Total: 2 x 2 x 4 = 16 experiments\n",
    "\n",
    "# queue_reduction， queue_abs， pressure， switch_penalty， throughput\n",
    "# reward_configs = [\n",
    "#     (\"default\", 1.0, 0.3, 0.15, 0.01, 0.005),\n",
    "#     (\"queue_focused\", 2.0, 0.5, 0.1, 0.05, 0.01),\n",
    "#     (\"pressure_focused\", 1.0, 0.2, 0.3, 0.05, 0.01),\n",
    "#     (\"throughput_focused\", 0.5, 0.2, 0.1, 0.05, 0.02),\n",
    "# ]\n",
    "\n",
    "reward_configs = [\n",
    "    (\"queue_focused\", 2.0, 0.5, 0.1, 0.05, 0.01),\n",
    "]\n",
    "\n",
    "noise_options = [True, False]\n",
    "\n",
    "train_model_configs = {\n",
    "    \"LR\": 3e-6,\n",
    "    \"N_STEPS\": 256,\n",
    "    \"N_EPOCHS\": 10,\n",
    "    \"MINI_BATCH_SIZE\": 64,\n",
    "    \"TOTAL_TIMESTEPS\": 100_00,\n",
    "    \"CLIP_EPS\": 0.2, \n",
    "    \"GAMMA\": 0.99, # Discount Factor\n",
    "    \"GAE_LAMBDA\": 0.95 # \n",
    "}\n",
    "\n",
    "results = run_experiments(\n",
    "    sumo_cmd=sumo_cmd,\n",
    "    tls_id=TLS_ID,\n",
    "    reward_configs=reward_configs,\n",
    "    noise_options=noise_options,\n",
    "    train_kwargs=train_model_configs,\n",
    "    eval_steps=1000,      # Evaluation steps per experiment\n",
    "    # train_timesteps=50000, # Training timesteps for RL agents (only used for RL experiments)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nTotal experiments completed: {len(results)}\")\n",
    "print(\"Check 'results/experiments_summary.csv' for summary of all experiments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>agent</th>\n",
       "      <th>avg_reward</th>\n",
       "      <th>avg_waiting_time</th>\n",
       "      <th>avg_queue_length</th>\n",
       "      <th>avg_pressure</th>\n",
       "      <th>avg_throughput</th>\n",
       "      <th>avg_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rl</td>\n",
       "      <td>-29.910735</td>\n",
       "      <td>0.626605</td>\n",
       "      <td>0.455325</td>\n",
       "      <td>98.661</td>\n",
       "      <td>0.553681</td>\n",
       "      <td>0.087363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rl</td>\n",
       "      <td>-29.911053</td>\n",
       "      <td>0.676176</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>98.473</td>\n",
       "      <td>0.547655</td>\n",
       "      <td>0.074868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>heuristic</td>\n",
       "      <td>-21.813613</td>\n",
       "      <td>0.737234</td>\n",
       "      <td>0.398313</td>\n",
       "      <td>71.845</td>\n",
       "      <td>0.548767</td>\n",
       "      <td>0.145065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>rl</td>\n",
       "      <td>-33.653914</td>\n",
       "      <td>0.610427</td>\n",
       "      <td>0.485225</td>\n",
       "      <td>111.063</td>\n",
       "      <td>0.553161</td>\n",
       "      <td>0.088292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id      agent  avg_reward  avg_waiting_time  avg_queue_length  \\\n",
       "0              1         rl  -29.910735          0.626605          0.455325   \n",
       "1              2         rl  -29.911053          0.676176          0.526000   \n",
       "2              3  heuristic  -21.813613          0.737234          0.398313   \n",
       "3              4         rl  -33.653914          0.610427          0.485225   \n",
       "\n",
       "   avg_pressure  avg_throughput  avg_speed  \n",
       "0        98.661        0.553681   0.087363  \n",
       "1        98.473        0.547655   0.074868  \n",
       "2        71.845        0.548767   0.145065  \n",
       "3       111.063        0.553161   0.088292  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "eva_results = pd.read_csv(\"results/experiments_summary.csv\")\n",
    "best_cols = [col for col in eva_results.columns if col.startswith(\"avg\")]\n",
    "df_best = eva_results[[\"experiment_id\", \"agent\"] + best_cols]\n",
    "\n",
    "df_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params  = {\n",
    "#     \"obs_dim\": env.observation_space.shape[0],\n",
    "#     \"act_dim\": env.action_space.n\n",
    "# }\n",
    "\n",
    "# ppo_model = ActorCritic(**model_params)\n",
    "# print(\"Model initialized. Starting training...\")\n",
    "\n",
    "# train_params = {\n",
    "#     \"model\": ppo_model,\n",
    "#     \"env\": env\n",
    "# }\n",
    "\n",
    "# model_hist = train_ppo(**train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
